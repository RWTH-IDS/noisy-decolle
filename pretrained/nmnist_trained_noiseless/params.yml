Nhid:
- 64
- 128
- 128
Mhid: []
alpha:
- 0.97
alpharp:
- 0.65
batch_size: 72
beta:
- 0.92
betas:
- 0.0
- 0.95
burnin_steps: 60
chunk_size_test: 300
chunk_size_train: 300
dataset: torchneuromorphic.nmnist.nmnist_dataloaders
deltat: 1000
input_shape:
- 2
- 32
- 32
kernel_size:
- 7
lc_ampl: 0.5
learning_rate: 1.e-3
loss: smoothL1
lr_drop_factor: 5
lr_drop_interval: 30
num_layers: 3
num_epochs: 100
num_conv_layers: 3
num_mlp_layers: 0
num_dl_workers: 4
optimizer: adamax
out_channels: 10
pool_size:
- 2
- 1
- 2
random_tau: true
reg_l:
- .0
- .0
- .0
- .0
test_interval: 1
init_theta:
- 5.e-5
- 5.e-5
- 5.e-5
- 5.e-5
online_update: True
learning_method: 'rtrl'
L2: 0.0 # Starting here, the values are added post running
ba_noise: 0.0
ba_noise_torch: 0.0
ba_train: 0.0
checkpoint_number: -1 # Start at last checkpoint
device: cuda:0
dropout: 0
gif_save_dir: null
hot_pixels: 0.0
initial_lr_drop: 0
log_folder: null
loss_scope: null
membrane_voltage_save_dir: null
mismatch: 0.0
mismatch_forward: 0.0
no_save: false
no_train: false
p_quantise: 0
params_file: parameters/params_nmnist.yml
percentile: 0
quant_method: float
quantise_bits: 0
reg1_l:
- 0.0
- 0.0
- 0.0
- 0.0
reg2_l:
- 0.0
- 0.0
- 0.0
- 0.0
reg_smooth_fct: None
resume_from: null
sam: null
sam_directory: null
save_dir: ''
save_voltage: null
seed: 1
spike_add: 0.0
spike_loss: 0.0
symmetric: false
thermal_noise: 0.0
threshold: 0.0
verbose: false
voltage_save_dir: null
with_output_layer: false